{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MacGyver MUD: Active Inference Essentials\n",
    "\n",
    "## The Fast Track to Understanding Active Inference\n",
    "\n",
    "**Version**: 1.0 \"Essentials\"\n",
    "\n",
    "**Estimated Time**: 45-60 minutes\n",
    "\n",
    "---\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "You're locked in a room with a door (might be locked) and a window (escape route, but costly). How should an intelligent agent decide what to do?\n",
    "\n",
    "This streamlined notebook covers:\n",
    "\n",
    "1. **Expected Free Energy (EFE)** - How agents score actions\n",
    "2. **Bayesian Belief Updates** - Learning from observations\n",
    "3. **The Silver Gauge** - Pythagorean means reveal k‚âà0 for all simple skills\n",
    "4. **Geometric Insights** - Why exploration-exploitation balance emerges naturally\n",
    "\n",
    "### Structure\n",
    "\n",
    "- **Interactive widgets** for hands-on exploration\n",
    "- **Real-time visualizations** of belief updates\n",
    "- **Checkpoints** to test understanding\n",
    "- **Neo4j integration** for live data (optional)\n",
    "\n",
    "---\n",
    "\n",
    "*\"The shortest path between two truths in the real domain passes through the complex domain.\" ‚Äî Jacques Hadamard*\n",
    "\n",
    "*We'll take the shortest path through Active Inference using geometry.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all utilities\n",
    "from macgyver_utils import *\n",
    "\n",
    "# Standard notebook setup\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úì Imports loaded\")\n",
    "print(\"‚úì Ready to explore Active Inference!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Neo4j (optional - will use fallback data if unavailable)\n",
    "initialize_neo4j(\n",
    "    uri=\"bolt://localhost:7687\",\n",
    "    user=\"neo4j\",\n",
    "    password=\"macgyver123\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display connection status\n",
    "display(get_connection_status())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start: The Challenge\n",
    "\n",
    "You wake up in a locked room with:\n",
    "- A **door** (unknown if locked or unlocked)\n",
    "- A **window** (guaranteed escape, but difficult/costly)\n",
    "\n",
    "Available skills:\n",
    "- `peek_door` - Observe the door lock (costs time, gains info)\n",
    "- `try_door` - Attempt to open door (costs more time, succeeds if unlocked)\n",
    "- `go_window` - Climb out window (costly but guaranteed)\n",
    "\n",
    "**The question**: What should you do first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: The Scenario\n",
    "\n",
    "Let's explore the state space and understand our options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Room Structure\n",
    "\n",
    "The room exists as a state graph in our Neo4j database. Each action transitions between states, and some transitions depend on the door being locked or unlocked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the room as a state graph\n",
    "visualize_room_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT 1: Initial understanding quiz\n",
    "create_quiz_widget(\n",
    "    question=\"If you don't know whether the door is locked, which skill gives you the most INFORMATION?\",\n",
    "    options=[\n",
    "        ('peek', 'peek_door - observe the lock'),\n",
    "        ('try', 'try_door - attempt to open'),\n",
    "        ('window', 'go_window - climb out window')\n",
    "    ],\n",
    "    correct_answer='peek',\n",
    "    feedback_dict={\n",
    "        'peek': 'Correct! Peeking is designed to gather information about the door state.',\n",
    "        'try': 'Try_door also gives info (through success/failure), but it\\'s more costly.',\n",
    "        'window': 'The window doesn\\'t tell you anything about the door.'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Beliefs\n",
    "\n",
    "Since we don't know if the door is locked, we maintain a **belief distribution** over possible states:\n",
    "\n",
    "- `P(locked)` = probability the door is locked\n",
    "- `P(unlocked)` = 1 - P(locked)\n",
    "\n",
    "This uncertainty drives our decision-making!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive belief slider with recommendation\n",
    "def update_belief_viz(change):\n",
    "    with output:\n",
    "        clear_output(wait=True)\n",
    "        belief = change['new']\n",
    "        plot_belief_distribution(belief)\n",
    "        plt.show()\n",
    "        show_recommendation(belief)\n",
    "\n",
    "slider, output = create_belief_slider(update_belief_viz, initial_value=0.5)\n",
    "display(slider, output)\n",
    "\n",
    "# Trigger initial display\n",
    "update_belief_viz({'new': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display skill properties as table\n",
    "skills_df = query_crisp_skills()\n",
    "display(HTML(\"<h3>Available Skills</h3>\"))\n",
    "display(skills_df.style.set_properties(**{'text-align': 'center'}).set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]}]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT 2: Skills understanding\n",
    "create_quiz_widget(\n",
    "    question=\"Which skill has the highest GOAL value (most reward if successful)?\",\n",
    "    options=[\n",
    "        ('peek', 'peek_door'),\n",
    "        ('try', 'try_door'),\n",
    "        ('window', 'go_window')\n",
    "    ],\n",
    "    correct_answer='try',\n",
    "    feedback_dict={\n",
    "        'peek': 'Peek has goal=0 because it doesn\\'t directly achieve the escape goal.',\n",
    "        'try': 'Correct! try_door has goal=10, the highest reward (if door is unlocked).',\n",
    "        'window': 'go_window has goal=8, good but less than try_door.'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 Summary\n",
    "\n",
    "Key insights:\n",
    "- We face **uncertainty** about the door state\n",
    "- Different skills offer different trade-offs: **cost vs goal vs information**\n",
    "- Our beliefs influence which action is best\n",
    "\n",
    "**Next**: How do we mathematically score actions under uncertainty?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Expected Free Energy\n",
    "\n",
    "How do we choose the \"best\" action when outcomes are uncertain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The EFE Formula\n",
    "\n",
    "Active Inference uses **Expected Free Energy (EFE)** to score actions:\n",
    "\n",
    "```\n",
    "EFE = Cost - Expected_Goal - Expected_Info\n",
    "```\n",
    "\n",
    "**Lower EFE is better** (less \"surprise\", more preferred)\n",
    "\n",
    "Components:\n",
    "- **Cost**: Resources consumed (time, energy)\n",
    "- **Expected Goal**: Reward √ó P(success)\n",
    "- **Expected Info**: Information gain (entropy reduction)\n",
    "\n",
    "The agent chooses the action with **minimum EFE**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EFE in Detail\n",
    "\n",
    "For `try_door`:\n",
    "- Cost = 2.0 (time to try)\n",
    "- Expected Goal = 10.0 √ó P(unlocked) = 10.0 √ó (1 - P(locked))\n",
    "- Expected Info = 0.0 (trying doesn't reduce uncertainty, it resolves it)\n",
    "\n",
    "For `peek_door`:\n",
    "- Cost = 1.0\n",
    "- Expected Goal = 0.0 (peeking doesn't achieve escape)\n",
    "- Expected Info = 0.8 (95% accurate observation)\n",
    "\n",
    "**The balance changes with belief!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive EFE calculator\n",
    "cost_input = create_float_input('Cost:', 2.0, min_val=0)\n",
    "goal_input = create_float_input('Goal Value:', 10.0, min_val=0)\n",
    "prob_input = widgets.FloatSlider(\n",
    "    value=0.5, min=0, max=1, step=0.05,\n",
    "    description='P(success):',\n",
    "    readout_format='.0%'\n",
    ")\n",
    "info_input = create_float_input('Info Gain:', 0.0, min_val=0)\n",
    "\n",
    "calc_button = widgets.Button(description='Calculate EFE', button_style='info')\n",
    "calc_output = widgets.Output()\n",
    "\n",
    "def calculate_efe(b):\n",
    "    with calc_output:\n",
    "        clear_output()\n",
    "        cost = cost_input.value\n",
    "        goal = goal_input.value\n",
    "        prob = prob_input.value\n",
    "        info = info_input.value\n",
    "        \n",
    "        expected_goal = goal * prob\n",
    "        efe = cost - expected_goal - info\n",
    "        \n",
    "        print(f\"\\nCalculation:\")\n",
    "        print(f\"  Cost:          {cost:.2f}\")\n",
    "        print(f\"  Expected Goal: {goal:.2f} √ó {prob:.2f} = {expected_goal:.2f}\")\n",
    "        print(f\"  Expected Info: {info:.2f}\")\n",
    "        print(f\"  \" + \"=\"*40)\n",
    "        print(f\"  EFE = {cost:.2f} - {expected_goal:.2f} - {info:.2f} = {efe:.2f}\")\n",
    "        print(f\"\\n{'Lower is better!' if efe < 0 else 'Higher cost than benefit'}\")\n",
    "\n",
    "calc_button.on_click(calculate_efe)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>EFE Calculator</h3>\"),\n",
    "    cost_input, goal_input, prob_input, info_input,\n",
    "    calc_button, calc_output\n",
    "]))\n",
    "\n",
    "# Auto-calculate on load\n",
    "calculate_efe(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: Score all skills at P(locked) = 50%\n",
    "print(\"\\n=\" * 50)\n",
    "print(\"SKILL SCORING DEMO: P(locked) = 50%\")\n",
    "print(\"=\" * 50)\n",
    "show_skill_scores(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT 3: Calculate EFE by hand\n",
    "create_quiz_widget(\n",
    "    question=\"If P(locked) = 0.8, what is the EFE for try_door? (Cost=2, Goal=10, Info=0)\",\n",
    "    options=[\n",
    "        ('zero', '0.0'),\n",
    "        ('neg_six', '-6.0'),\n",
    "        ('pos_zero', '+0.0'),\n",
    "        ('pos_zero_two', '+0.2')\n",
    "    ],\n",
    "    correct_answer='pos_zero',\n",
    "    feedback_dict={\n",
    "        'zero': 'Close! Remember: EFE = Cost - (Goal √ó P(unlocked)) - Info',\n",
    "        'neg_six': 'That would be if P(unlocked)=0.8. Remember P(unlocked)=1-P(locked)=0.2',\n",
    "        'pos_zero': 'Correct! EFE = 2 - (10 √ó 0.2) - 0 = 2 - 2 = 0.0',\n",
    "        'pos_zero_two': 'Close! Check your calculation of Expected Goal.'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive: How belief changes skill preference\n",
    "def update_skill_scores(change):\n",
    "    with scores_output:\n",
    "        clear_output(wait=True)\n",
    "        belief = change['new']\n",
    "        show_skill_scores(belief)\n",
    "\n",
    "belief_slider, scores_output = create_belief_slider(update_skill_scores, initial_value=0.5)\n",
    "\n",
    "display(widgets.HTML(\"<h3>How Belief Affects Skill Selection</h3>\"))\n",
    "display(widgets.HTML(\"<p>Move the slider to see how skill preferences change with belief:</p>\"))\n",
    "display(belief_slider, scores_output)\n",
    "\n",
    "# Trigger initial display\n",
    "update_skill_scores({'new': 0.5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: EFE across belief range\n",
    "beliefs = np.linspace(0, 1, 100)\n",
    "skills = [\n",
    "    {'name': 'peek_door', 'cost': 1.0, 'goal': 0.0, 'info_gain': 0.8},\n",
    "    {'name': 'try_door', 'cost': 2.0, 'goal': 10.0, 'info_gain': 0.0},\n",
    "    {'name': 'go_window', 'cost': 5.0, 'goal': 8.0, 'info_gain': 0.0}\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for skill in skills:\n",
    "    efes = [score_skill(skill, b) for b in beliefs]\n",
    "    ax.plot(beliefs, efes, label=skill['name'], linewidth=2.5)\n",
    "\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5, label='Break-even')\n",
    "style_plot(ax, 'EFE vs Belief: When to Explore vs Exploit', \n",
    "           'P(locked)', 'Expected Free Energy')\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "ax.invert_yaxis()  # Lower is better\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüí° Notice: At low P(locked), try_door dominates (exploitation)\")\n",
    "print(\"          At high P(locked), peek_door wins (exploration)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>üîç Math Deep-Dive: Why This Formula? (Click to expand)</strong></summary>\n",
    "\n",
    "### The Free Energy Principle\n",
    "\n",
    "EFE comes from the **Free Energy Principle** in neuroscience/AI:\n",
    "\n",
    "**Key idea**: Agents minimize surprise (unexpected observations)\n",
    "\n",
    "Free Energy is a computationally tractable upper bound on surprise:\n",
    "\n",
    "```\n",
    "F ‚âà -ln P(observations | model)\n",
    "```\n",
    "\n",
    "For **future actions**, we can't observe yet, so we use **expected** free energy:\n",
    "\n",
    "```\n",
    "G(œÄ) = E[Cost] - E[Goal Achievement] - E[Information Gain]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **E[Cost]** = Pragmatic cost (energy, time)\n",
    "- **E[Goal]** = Expected goal achievement (exploitation)\n",
    "- **E[Info]** = Expected information gain (exploration)\n",
    "\n",
    "This naturally balances **exploration** (learn about world) vs **exploitation** (achieve goals)!\n",
    "\n",
    "### Connection to Control Theory\n",
    "\n",
    "EFE is related to optimal control's \"cost-to-go\" but adds epistemic value (information).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights: EFE\n",
    "\n",
    "1. **EFE balances three factors**: cost, goal, and information\n",
    "2. **Beliefs matter**: Same action has different EFE depending on what you believe\n",
    "3. **Automatic exploration**: High uncertainty ‚Üí information becomes valuable\n",
    "4. **Automatic exploitation**: High certainty ‚Üí go for the goal\n",
    "\n",
    "**This is the core of Active Inference decision-making!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 Summary\n",
    "\n",
    "We've learned:\n",
    "- How to calculate EFE for any action\n",
    "- Why beliefs change which action is preferred\n",
    "- How exploration and exploitation emerge naturally\n",
    "\n",
    "**Next**: How do beliefs update when we observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 3: Bayesian Belief Updates\n",
    "\n",
    "Actions change our beliefs. How?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Theorem for Belief Updates\n",
    "\n",
    "When we observe something, we update our beliefs using **Bayes' Theorem**:\n",
    "\n",
    "```\n",
    "P(state | observation) = P(observation | state) √ó P(state) / P(observation)\n",
    "```\n",
    "\n",
    "In words:\n",
    "- **Posterior** = (Likelihood √ó Prior) / Evidence\n",
    "\n",
    "Example: If we peek and see \"locked\":\n",
    "- Prior: P(locked) = 0.5\n",
    "- Likelihood: P(see \"locked\" | actually locked) = 0.95\n",
    "- Posterior: P(locked | saw \"locked\") = ?\n",
    "\n",
    "Let's calculate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Update Formula in Action\n",
    "\n",
    "For our peek_door skill (95% accurate):\n",
    "\n",
    "**If we observe \"locked\":**\n",
    "```\n",
    "P(locked | obs_locked) = [0.95 √ó P(locked)] / [0.95 √ó P(locked) + 0.05 √ó P(unlocked)]\n",
    "```\n",
    "\n",
    "**If we observe \"unlocked\":**\n",
    "```\n",
    "P(locked | obs_unlocked) = [0.05 √ó P(locked)] / [0.05 √ó P(locked) + 0.95 √ó P(unlocked)]\n",
    "```\n",
    "\n",
    "Notice: Observing \"locked\" increases P(locked), observing \"unlocked\" decreases it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive belief update simulator\n",
    "prior_slider = widgets.FloatSlider(\n",
    "    value=0.5, min=0, max=1, step=0.05,\n",
    "    description='Prior P(locked):',\n",
    "    readout_format='.0%',\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "true_state_select = widgets.Dropdown(\n",
    "    options=[('Locked', 'locked'), ('Unlocked', 'unlocked')],\n",
    "    value='locked',\n",
    "    description='True State:',\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "action_select = widgets.Dropdown(\n",
    "    options=[('Peek Door', 'peek_door'), ('Try Door', 'try_door')],\n",
    "    value='peek_door',\n",
    "    description='Action:',\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "sim_button = widgets.Button(description='Simulate Update', button_style='success', icon='play')\n",
    "sim_output = widgets.Output()\n",
    "\n",
    "def run_simulation(b):\n",
    "    with sim_output:\n",
    "        clear_output(wait=True)\n",
    "        prior = prior_slider.value\n",
    "        true_state = true_state_select.value\n",
    "        action = action_select.value\n",
    "        \n",
    "        posterior, obs = simulate_belief_update(prior, true_state, action)\n",
    "        \n",
    "        plot_3panel_update(prior, action, obs, posterior)\n",
    "        \n",
    "        print(f\"\\nüìä Update Summary:\")\n",
    "        print(f\"  Prior:      P(locked) = {prior:.0%}\")\n",
    "        print(f\"  Action:     {action}\")\n",
    "        print(f\"  Observed:   {obs}\")\n",
    "        print(f\"  Posterior:  P(locked) = {posterior:.0%}\")\n",
    "        print(f\"  Change:     {posterior - prior:+.0%}\")\n",
    "\n",
    "sim_button.on_click(run_simulation)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Bayesian Belief Update Simulator</h3>\"),\n",
    "    widgets.HTML(\"<p>Set parameters and click simulate to see belief updates:</p>\"),\n",
    "    prior_slider,\n",
    "    true_state_select,\n",
    "    action_select,\n",
    "    sim_button,\n",
    "    sim_output\n",
    "]))\n",
    "\n",
    "# Auto-run on load\n",
    "run_simulation(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT 4: Calculate posterior by hand\n",
    "create_quiz_widget(\n",
    "    question=\"If P(locked)=0.5 and we peek and observe 'locked', what's the new P(locked)? (95% accurate peek)\",\n",
    "    options=[\n",
    "        ('fifty', '0.50 (no change)'),\n",
    "        ('ninety', '0.90'),\n",
    "        ('ninety_five', '0.95 (perfect certainty)'),\n",
    "        ('seventy', '0.70')\n",
    "    ],\n",
    "    correct_answer='ninety_five',\n",
    "    feedback_dict={\n",
    "        'fifty': 'Beliefs should change with new evidence!',\n",
    "        'ninety': 'Close! But with 50/50 prior and 95% accuracy, we get stronger update.',\n",
    "        'ninety_five': 'Correct! P(locked|obs) = (0.95 √ó 0.5) / (0.95 √ó 0.5 + 0.05 √ó 0.5) = 0.95',\n",
    "        'seventy': 'Too conservative. The observation is 95% reliable!'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full episode simulation\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FULL EPISODE SIMULATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nScenario: Door is actually LOCKED, agent starts with P(locked)=0.5\")\n",
    "print(\"Let's watch Active Inference in action...\\n\")\n",
    "\n",
    "episode = simulate_episode(initial_belief=0.5, true_door_state='locked', max_steps=5)\n",
    "\n",
    "for step in episode:\n",
    "    print(f\"\\n--- Step {step['step']} ---\")\n",
    "    print(f\"Belief: P(locked) = {step['belief_locked']:.0%}\")\n",
    "    print(f\"Scores: peek={step['scores']['peek_door']:.2f}, try={step['scores']['try_door']:.2f}\")\n",
    "    print(f\"Action: {step['action']}\")\n",
    "    print(f\"Result: {step['observation']}\")\n",
    "    print(f\"New Belief: P(locked) = {step['new_belief']:.0%}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize episode progression\n",
    "if len(episode) > 0:\n",
    "    beliefs = [episode[0]['belief_locked']] + [s['new_belief'] for s in episode]\n",
    "    steps = list(range(len(beliefs)))\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(steps, beliefs, marker='o', linewidth=3, markersize=10, color=COLORS['info'])\n",
    "    \n",
    "    # Annotate actions\n",
    "    for i, step in enumerate(episode):\n",
    "        ax.annotate(step['action'], \n",
    "                   xy=(i+0.5, (beliefs[i] + beliefs[i+1])/2),\n",
    "                   ha='center', fontsize=9,\n",
    "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
    "    \n",
    "    style_plot(ax, 'Belief Evolution Through Episode', 'Step', 'P(locked)')\n",
    "    ax.set_ylim(-0.05, 1.05)\n",
    "    ax.axhline(y=0.5, color='gray', linestyle='--', alpha=0.3, label='Maximum uncertainty')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Episode\n",
    "\n",
    "Watch what happens:\n",
    "\n",
    "1. **Initial uncertainty** (P(locked) ‚âà 0.5) ‚Üí Agent chooses to **explore** (peek)\n",
    "2. **After peeking**, belief updates dramatically (either ~0.95 or ~0.05)\n",
    "3. **With high certainty**, agent switches to **exploitation** (try_door)\n",
    "\n",
    "This is **Active Inference** in action:\n",
    "- No hard-coded \"explore first, then exploit\"\n",
    "- Behavior emerges from minimizing EFE\n",
    "- Information-seeking is automatic when uncertain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Peek vs Try?\n",
    "\n",
    "The crossover point depends on the **EFE balance**:\n",
    "\n",
    "- **High uncertainty** ‚Üí Info gain is valuable ‚Üí Peek wins\n",
    "- **Low uncertainty** ‚Üí Goal achievement dominates ‚Üí Try wins\n",
    "\n",
    "The threshold typically sits around **P(locked) ‚âà 0.2-0.3** for these skill values.\n",
    "\n",
    "**Key insight**: The decision threshold emerges from the math, not from rules!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 Summary\n",
    "\n",
    "We've learned:\n",
    "- How Bayesian updates change beliefs based on observations\n",
    "- How EFE + belief updates create explore-exploit behavior\n",
    "- How to simulate complete episodes\n",
    "\n",
    "**Next**: The geometric revelation that changes everything!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: The Silver Gauge Revelation\n",
    "\n",
    "We can score actions. We can update beliefs. But something's missing..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Missing?\n",
    "\n",
    "**Question**: Are our current skills well-balanced?\n",
    "\n",
    "Consider:\n",
    "- `peek_door`: ALL info, NO goal\n",
    "- `try_door`: ALL goal, NO info\n",
    "- `go_window`: ALL goal, NO info\n",
    "\n",
    "These are **extreme specialists**! Is there a mathematical way to measure this imbalance?\n",
    "\n",
    "Enter: **Pythagorean Means** (circa 500 BCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythagorean Means: Ancient Math, Modern Insights\n",
    "\n",
    "For two positive numbers a and b:\n",
    "\n",
    "```\n",
    "Harmonic Mean (HM)  = 2 / (1/a + 1/b)     [rewards balance]\n",
    "Geometric Mean (GM) = ‚àö(a √ó b)             [multiplicative center]\n",
    "Arithmetic Mean (AM) = (a + b) / 2         [average]\n",
    "```\n",
    "\n",
    "**Key property**: HM ‚â§ GM ‚â§ AM (always!)\n",
    "\n",
    "**When equal**: Only when a = b (perfect balance)\n",
    "\n",
    "**The closer GM is to AM, the more balanced a and b are!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Pythagorean means calculator\n",
    "a_input = widgets.FloatSlider(value=5.0, min=0.1, max=10, step=0.1, description='Value a:', readout_format='.1f')\n",
    "b_input = widgets.FloatSlider(value=5.0, min=0.1, max=10, step=0.1, description='Value b:', readout_format='.1f')\n",
    "\n",
    "means_button = widgets.Button(description='Calculate Means', button_style='info', icon='calculator')\n",
    "means_output = widgets.Output()\n",
    "\n",
    "def calc_means(b):\n",
    "    with means_output:\n",
    "        clear_output(wait=True)\n",
    "        a = a_input.value\n",
    "        b_val = b_input.value\n",
    "        result = plot_pythagorean_means(a, b_val)\n",
    "        if result:\n",
    "            print(f\"\\nüìê Means Analysis:\")\n",
    "            print(f\"  HM = {result['HM']:.3f}\")\n",
    "            print(f\"  GM = {result['GM']:.3f}\")\n",
    "            print(f\"  AM = {result['AM']:.3f}\")\n",
    "            print(f\"\\n  k = GM/AM = {result['k']:.3f}\")\n",
    "            print(f\"  Balance: {result['balance']}\")\n",
    "            print(f\"\\nüí° k close to 1.0 means a and b are balanced!\")\n",
    "\n",
    "means_button.on_click(calc_means)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Pythagorean Means Explorer</h3>\"),\n",
    "    widgets.HTML(\"<p>Try different values to see how balance affects the means:</p>\"),\n",
    "    a_input, b_input,\n",
    "    means_button,\n",
    "    means_output\n",
    "]))\n",
    "\n",
    "# Auto-calculate\n",
    "calc_means(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Inequality and k-Coefficient\n",
    "\n",
    "Define the **balance coefficient**:\n",
    "\n",
    "```\n",
    "k = GM / AM\n",
    "```\n",
    "\n",
    "Properties:\n",
    "- **k = 1.0**: Perfect balance (a = b)\n",
    "- **k < 1.0**: Imbalance (the further from 1, the worse)\n",
    "- **k ‚Üí 0**: Extreme imbalance (one value near zero)\n",
    "\n",
    "**This gives us a quantitative measure of balance!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The k‚âà0 Revelation\n",
    "\n",
    "Now apply this to our skills:\n",
    "\n",
    "**For exploration-exploitation balance**:\n",
    "```\n",
    "k_explore = GM(goal, info_gain) / AM(goal, info_gain)\n",
    "```\n",
    "\n",
    "Let's calculate for our skills:\n",
    "- `peek_door`: goal=0, info=0.8 ‚Üí k‚âà0 (IMBALANCED)\n",
    "- `try_door`: goal=10, info=0 ‚Üí k‚âà0 (IMBALANCED)\n",
    "\n",
    "**All our simple skills are specialists with k‚âà0!**\n",
    "\n",
    "This is not a bug‚Äîit's a **feature**. Simple skills are naturally specialists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silver Gauge for all crisp skills\n",
    "silver_df = calculate_silver_for_crisp()\n",
    "\n",
    "display(HTML(\"<h3>Silver Gauge Analysis: Crisp Skills</h3>\"))\n",
    "display(HTML(\"<p>k_explore measures goal vs info_gain balance (closer to 1.0 = more balanced):</p>\"))\n",
    "\n",
    "# Format for display\n",
    "display_df = silver_df.copy()\n",
    "display_df['k_explore'] = display_df['k_explore'].apply(\n",
    "    lambda x: f\"{x:.3f}\" if not pd.isna(x) else \"N/A\"\n",
    ")\n",
    "\n",
    "display(display_df.style.set_properties(**{'text-align': 'center'}).set_table_styles(\n",
    "    [{'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]}]\n",
    "))\n",
    "\n",
    "print(\"\\nüéØ Key Observation: k_explore is N/A (undefined) when goal=0 OR info=0\")\n",
    "print(\"   This means ALL simple skills are extreme specialists!\")\n",
    "print(\"\\nüí° To get k‚âà1.0, we need skills with BOTH goal AND info_gain > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results: Why k‚âà0 for Simple Skills\n",
    "\n",
    "Mathematical reality:\n",
    "```\n",
    "If goal = 0:  GM = ‚àö(0 √ó info) = 0  ‚Üí  k = 0/AM = 0\n",
    "If info = 0:  GM = ‚àö(goal √ó 0) = 0  ‚Üí  k = 0/AM = 0\n",
    "```\n",
    "\n",
    "**Implication**: You cannot have a balanced skill unless BOTH objectives are non-zero!\n",
    "\n",
    "This means:\n",
    "- Simple single-purpose skills are always specialists (k‚âà0)\n",
    "- To get balance, you need **blended skills** (goal > 0 AND info > 0)\n",
    "\n",
    "**This is the geometric diagnostic we needed!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKPOINT 5: Design challenge\n",
    "create_quiz_widget(\n",
    "    question=\"To create a skill with k_explore ‚âà 0.9 (well-balanced), what properties should it have?\",\n",
    "    options=[\n",
    "        ('high_goal', 'High goal, zero info_gain'),\n",
    "        ('high_info', 'Zero goal, high info_gain'),\n",
    "        ('both_high', 'Both goal and info_gain > 0, similar magnitudes'),\n",
    "        ('both_zero', 'Both goal and info_gain = 0')\n",
    "    ],\n",
    "    correct_answer='both_high',\n",
    "    feedback_dict={\n",
    "        'high_goal': 'This gives k‚âà0 (specialist). Need BOTH objectives > 0.',\n",
    "        'high_info': 'This gives k‚âà0 (specialist). Need BOTH objectives > 0.',\n",
    "        'both_high': 'Correct! k = GM/AM is close to 1 when values are similar. Need goal‚âàinfo_gain.',\n",
    "        'both_zero': 'This is undefined (0/0). Need positive values!'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Works: The Geometric Insight\n",
    "\n",
    "The **Silver Gauge** (k-coefficient from Pythagorean means) works because:\n",
    "\n",
    "1. **Geometric mean rewards balance**: ‚àö(a √ó b) is maximized when a ‚âà b\n",
    "2. **Arithmetic mean is indifferent**: (a + b)/2 treats all combinations equally\n",
    "3. **The ratio k = GM/AM captures the gap**: Perfect balance ‚Üí k=1, imbalance ‚Üí k‚Üí0\n",
    "\n",
    "This is a **diagnostic metric**:\n",
    "- Reveals what's missing (balanced skills)\n",
    "- Quantifies the gap\n",
    "- Guides design (create skills with k‚âà1)\n",
    "\n",
    "**Ancient geometry solving modern AI problems!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Implications\n",
    "\n",
    "This geometric insight leads to:\n",
    "\n",
    "1. **Skill Design**: Create \"blended\" skills that balance objectives\n",
    "   - Example: `smart_try_door` with goal=7, info=6 ‚Üí k‚âà0.98\n",
    "\n",
    "2. **Multi-Objective Optimization**: Use k to measure trade-offs\n",
    "   - Not just goal vs info, but any competing objectives\n",
    "   - k_efficiency = balance of benefit vs cost\n",
    "\n",
    "3. **Diagnostic-Driven Design**: \n",
    "   - Measure what exists (k‚âà0 for specialists)\n",
    "   - Identify gaps (need k‚âà1 skills)\n",
    "   - Design solutions (create balanced skills)\n",
    "\n",
    "4. **Evolutionary Perspective**:\n",
    "   - Simple skills are specialists (k‚âà0)\n",
    "   - Evolution fills the Pareto front with balanced skills (k‚Üí1)\n",
    "   - Diversity emerges from geometry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Climax: Geometric Balance\n",
    "\n",
    "**The full picture**:\n",
    "\n",
    "```\n",
    "Simple Skills (k‚âà0):           Blended Skills (k‚âà1):\n",
    "‚îú‚îÄ peek (info only)           ‚îú‚îÄ smart_peek (goal=2, info=7)\n",
    "‚îú‚îÄ try (goal only)            ‚îú‚îÄ careful_try (goal=8, info=5)\n",
    "‚îî‚îÄ window (goal only)         ‚îî‚îÄ observe_window (goal=6, info=6)\n",
    "\n",
    "Diagnostic reveals gap  ‚Üí  Design fills gap  ‚Üí  Emergent diversity\n",
    "```\n",
    "\n",
    "**This is how Active Inference + Geometric Diagnostics = Innovation**\n",
    "\n",
    "The math tells us:\n",
    "- What we have (specialists)\n",
    "- What we're missing (balanced skills)\n",
    "- How to measure success (k‚Üí1)\n",
    "\n",
    "**Geometry as design compass!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 Summary\n",
    "\n",
    "We've discovered:\n",
    "- Pythagorean means measure balance between objectives\n",
    "- k-coefficient quantifies this balance (k=1 perfect, k‚Üí0 imbalanced)\n",
    "- ALL simple skills have k‚âà0 (mathematical necessity)\n",
    "- This diagnostic reveals the need for blended skills\n",
    "- Geometry guides evolution toward diversity\n",
    "\n",
    "**This is the Silver Gauge revelation: ancient math illuminating modern AI!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 5: Explore & Next Steps\n",
    "\n",
    "Now that you understand the core concepts, let's explore the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neo4j Playground\n",
    "\n",
    "If you're connected to Neo4j, you can run custom queries to explore the graph.\n",
    "\n",
    "The database contains:\n",
    "- **States**: Room states (stuck, escaped, etc.)\n",
    "- **Skills**: Both crisp and blended skills\n",
    "- **Transitions**: How skills move between states\n",
    "- **Properties**: cost, goal, info_gain, k_explore, k_efficiency\n",
    "\n",
    "Try the queries below to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive query widget\n",
    "query_input = widgets.Textarea(\n",
    "    value=\"MATCH (s:Skill) RETURN s.name, s.cost, s.goal, s.info_gain LIMIT 5\",\n",
    "    description='Cypher Query:',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "query_button = widgets.Button(description='Run Query', button_style='primary', icon='database')\n",
    "query_output = widgets.Output()\n",
    "\n",
    "def run_custom_query(b):\n",
    "    with query_output:\n",
    "        clear_output()\n",
    "        query = query_input.value\n",
    "        \n",
    "        if not NEO4J_CONNECTED:\n",
    "            print(\"‚ùå Not connected to Neo4j. Connect first!\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Running query...\\n\")\n",
    "        results = run_query(query)\n",
    "        \n",
    "        if results:\n",
    "            df = pd.DataFrame(results)\n",
    "            display(df)\n",
    "            print(f\"\\n‚úì Returned {len(results)} rows\")\n",
    "        else:\n",
    "            print(\"No results returned (or query error)\")\n",
    "\n",
    "query_button.on_click(run_custom_query)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h3>Neo4j Query Playground</h3>\"),\n",
    "    query_input,\n",
    "    query_button,\n",
    "    query_output\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try These Queries\n",
    "\n",
    "**1. Find all blended skills (both goal and info_gain > 0):**\n",
    "```cypher\n",
    "MATCH (s:Skill)\n",
    "WHERE s.goal > 0 AND s.info_gain > 0\n",
    "RETURN s.name, s.goal, s.info_gain, s.k_explore\n",
    "ORDER BY s.k_explore DESC\n",
    "```\n",
    "\n",
    "**2. Find most balanced skills (k_explore closest to 1):**\n",
    "```cypher\n",
    "MATCH (s:Skill)\n",
    "WHERE s.k_explore IS NOT NULL\n",
    "RETURN s.name, s.k_explore\n",
    "ORDER BY s.k_explore DESC\n",
    "LIMIT 5\n",
    "```\n",
    "\n",
    "**3. Explore state transitions:**\n",
    "```cypher\n",
    "MATCH (s:State)-[r]->(t:State)\n",
    "RETURN s.name AS from, type(r) AS via, t.name AS to\n",
    "```\n",
    "\n",
    "**4. Find specialist skills (k_explore near 0 or NULL):**\n",
    "```cypher\n",
    "MATCH (s:Skill)\n",
    "WHERE s.goal = 0 OR s.info_gain = 0\n",
    "RETURN s.name, s.goal, s.info_gain, 'specialist' AS type\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Go Deeper\n",
    "\n",
    "Want to explore more? Check out:\n",
    "\n",
    "### 1. The Full Deep Dive Notebook\n",
    "- `MacGyverMUD_DeepDive.ipynb` (89 cells)\n",
    "- Detailed mathematical derivations\n",
    "- Multi-objective evolution\n",
    "- Advanced visualizations\n",
    "\n",
    "### 2. Extend the Code\n",
    "- Design new blended skills\n",
    "- Implement different belief update rules\n",
    "- Add more state complexity\n",
    "- Visualize Pareto fronts\n",
    "\n",
    "### 3. Active Inference Literature\n",
    "- Friston et al. (2015) - \"Active Inference and Learning\"\n",
    "- Parr & Friston (2019) - \"Working Memory, Attention, and Salience\"\n",
    "- Da Costa et al. (2020) - \"Active Inference on Discrete State Spaces\"\n",
    "\n",
    "### 4. Pythagorean Means\n",
    "- Classic inequality proofs\n",
    "- Applications in economics, physics, engineering\n",
    "- Generalized means (power means)\n",
    "\n",
    "### 5. Multi-Objective Optimization\n",
    "- Pareto efficiency\n",
    "- NSGA-II and NSGA-III algorithms\n",
    "- Hypervolume indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources & Wrap-Up\n",
    "\n",
    "### What We've Learned\n",
    "\n",
    "1. **Active Inference**: Agents minimize Expected Free Energy\n",
    "   - EFE = Cost - Expected_Goal - Expected_Info\n",
    "   - Naturally balances exploration and exploitation\n",
    "\n",
    "2. **Bayesian Updates**: Beliefs change with observations\n",
    "   - P(state | obs) ‚àù P(obs | state) √ó P(state)\n",
    "   - Information reduces uncertainty\n",
    "\n",
    "3. **Pythagorean Means**: Ancient geometry, modern insights\n",
    "   - k = GM/AM measures balance\n",
    "   - k‚âà0 for specialists, k‚âà1 for balanced skills\n",
    "\n",
    "4. **Diagnostic-Driven Design**: Geometry guides evolution\n",
    "   - Measure what exists\n",
    "   - Identify gaps\n",
    "   - Design solutions\n",
    "\n",
    "### Core Insight\n",
    "\n",
    "> **Simple skills are necessarily specialists (k‚âà0). To achieve balance (k‚âà1), you must blend objectives. This geometric diagnostic reveals the path to innovation.**\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- **Experiment**: Modify skills, change beliefs, run episodes\n",
    "- **Explore**: Query the Neo4j database\n",
    "- **Extend**: Design your own blended skills\n",
    "- **Learn More**: Dive into the full notebook and literature\n",
    "\n",
    "### Acknowledgments\n",
    "\n",
    "This notebook builds on:\n",
    "- Active Inference framework (Karl Friston et al.)\n",
    "- Pythagorean mathematics (ancient Greeks)\n",
    "- Multi-objective optimization theory\n",
    "- Graph database technology (Neo4j)\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for exploring Active Inference with us!**\n",
    "\n",
    "*\"The only way to learn mathematics is to do mathematics.\" ‚Äî Paul Halmos*\n",
    "\n",
    "*Now go experiment!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
